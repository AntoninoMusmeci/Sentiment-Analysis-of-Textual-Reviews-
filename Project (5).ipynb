{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "axUF7fIralRv"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import cm\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "PRINT = True\n",
    "\n",
    "def SVM_gridsearch(X_train,X_val):\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    # Set the parameters by cross-validation\n",
    "    tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-2,1e-1,1,10],\n",
    "                         'C': [0.1,1, 10]},\n",
    "                        {'kernel': ['linear'], 'C': [0.1,1, 10]}]\n",
    "\n",
    "    scores = ['f1_weighted']\n",
    "\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring='f1_weighted' , cv = 3\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "   \n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    print( clf.cv_results_['mean_test_score'])\n",
    "    y_true, y_pred = y_val, clf.predict(X_val)\n",
    "    print(f1_score(y_true, y_pred, average='weighted'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "mLLkyFuWa6Of",
    "outputId": "ccddc5ca-932e-4c58-80d7-8e281d7ac879"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "ItalianStemmer=SnowballStemmer(\"italian\")\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "         self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, document):\n",
    "        lemmas = []\n",
    "\n",
    "        words = WordPunctTokenizer().tokenize(document)\n",
    "        for t in words:\n",
    "        \n",
    "            if len(t)>3:\n",
    "                t = t.strip()\n",
    "                if any(letter.isalpha() == False for letter in t):\n",
    "                    continue;\n",
    "                lemma = ItalianStemmer.stem(t)\n",
    "                lemmas.append(lemma)\n",
    " \n",
    "        return lemmas\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9whi-vf-bX5v",
    "outputId": "2c03c435-a3fd-4f6e-ae83-39d7111fa39f"
   },
   "outputs": [],
   "source": [
    "words = [\"a\",\"abbastanza\",\"abbia\",\"abbiamo\",\"abbiano\",\"abbiate\",\"accidenti\",\"ad\",\"adesso\",\"affinche\",\"agl\",\"agli\",\"ahime\",\"ahimã¨\",\"ahimè\",\"ai\",\"al\",\"alcuna\",\"alcuni\",\"alcuno\",\"all\",\"alla\",\"alle\",\"allo\",\"allora\",\"altre\",\"altri\",\"altrimenti\",\"altro\",\"altrove\",\"altrui\",\"anche\",\"ancora\",\"anni\",\"anno\",\"ansa\",\"anticipo\",\"assai\",\"attesa\",\"attraverso\",\"avanti\",\"avemmo\",\"avendo\",\"avente\",\"aver\",\"avere\",\"averlo\",\"avesse\",\"avessero\",\"avessi\",\"avessimo\",\"aveste\",\"avesti\",\"avete\",\"aveva\",\"avevamo\",\"avevano\",\"avevate\",\"avevi\",\"avevo\",\"avrai\",\"avranno\",\"avrebbe\",\"avrebbero\",\"avrei\",\"avremmo\",\"avremo\",\"avreste\",\"avresti\",\"avrete\",\"avrà\",\"avrò\",\"avuta\",\"avute\",\"avuti\",\"avuto\",\"basta\",\"ben\",\"bene\",\"benissimo\",\"berlusconi\",\"brava\",\"bravo\",\"buono\",\"c\",\"casa\",\"caso\",\"cento\",\"certa\",\"certe\",\"certi\",\"certo\",\"che\",\"chi\",\"chicchessia\",\"chiunque\",\"ci\",\"ciascuna\",\"ciascuno\",\"cima\",\"cinque\",\"cio\",\"cioe\",\"cioã¨\",\"cioè\",\"circa\",\"citta\",\"città\",\"cittã\",\"ciã²\",\"ciò\",\"co\",\"codesta\",\"codesti\",\"codesto\",\"cogli\",\"coi\",\"col\",\"colei\",\"coll\",\"coloro\",\"colui\",\"come\",\"cominci\",\"comprare\",\"comunque\",\"con\",\"concernente\",\"conciliarsi\",\"conclusione\",\"consecutivi\",\"consecutivo\",\"consiglio\",\"contro\",\"cortesia\",\"cos\",\"cosa\",\"cosi\",\"cosã¬\",\"così\",\"cui\",\"d\",\"da\",\"dagl\",\"dagli\",\"dai\",\"dal\",\"dall\",\"dalla\",\"dalle\",\"dallo\",\"dappertutto\",\"davanti\",\"degl\",\"degli\",\"dei\",\"del\",\"dell\",\"della\",\"delle\",\"dello\",\"dentro\",\"detto\",\"deve\",\"devo\",\"di\",\"dice\",\"dietro\",\"dire\",\"dirimpetto\",\"diventa\",\"diventare\",\"diventato\",\"dopo\",\"doppio\",\"dov\",\"dove\",\"dovra\",\"dovrà\",\"dovrã\",\"dovunque\",\"due\",\"dunque\",\"durante\",\"e\",\"ebbe\",\"ebbero\",\"ebbi\",\"ecc\",\"ecco\",\"ed\",\"effettivamente\",\"egli\",\"ella\",\"entrambi\",\"eppure\",\"era\",\"erano\",\"eravamo\",\"eravate\",\"eri\",\"ero\",\"esempio\",\"esse\",\"essendo\",\"esser\",\"essere\",\"essi\",\"ex\",\"fa\",\"faccia\",\"facciamo\",\"facciano\",\"facciate\",\"faccio\",\"facemmo\",\"facendo\",\"facesse\",\"facessero\",\"facessi\",\"facessimo\",\"faceste\",\"facesti\",\"faceva\",\"facevamo\",\"facevano\",\"facevate\",\"facevi\",\"facevo\",\"fai\",\"fanno\",\"farai\",\"faranno\",\"fare\",\"farebbe\",\"farebbero\",\"farei\",\"faremmo\",\"faremo\",\"fareste\",\"faresti\",\"farete\",\"farà\",\"farò\",\"fatto\",\"favore\",\"fece\",\"fecero\",\"feci\",\"fin\",\"finalmente\",\"finche\",\"fine\",\"fino\",\"forse\",\"forza\",\"fosse\",\"fossero\",\"fossi\",\"fossimo\",\"foste\",\"fosti\",\"fra\",\"frattempo\",\"fu\",\"fui\",\"fummo\",\"fuori\",\"furono\",\"futuro\",\"generale\",\"gente\",\"gia\",\"giacche\",\"giorni\",\"giorno\",\"giu\",\"già\",\"giã\",\"gli\",\"gliela\",\"gliele\",\"glieli\",\"glielo\",\"gliene\",\"governo\",\"grande\",\"grazie\",\"gruppo\",\"ha\",\"haha\",\"hai\",\"hanno\",\"ho\",\"i\",\"ie\",\"ieri\",\"il\",\"improvviso\",\"in\",\"inc\",\"indietro\",\"infatti\",\"inoltre\",\"insieme\",\"intanto\",\"intorno\",\"invece\",\"io\",\"l\",\"la\",\"lasciato\",\"lato\",\"lavoro\",\"le\",\"lei\",\"li\",\"lo\",\"lontano\",\"loro\",\"lui\",\"lungo\",\"luogo\",\"là\",\"lã\",\"ma\",\"macche\",\"magari\",\"maggior\",\"mai\",\"male\",\"malgrado\",\"malissimo\",\"mancanza\",\"marche\",\"me\",\"medesimo\",\"mediante\",\"meglio\",\"meno\",\"mentre\",\"mesi\",\"mezzo\",\"mi\",\"mia\",\"mie\",\"miei\",\"mila\",\"miliardi\",\"milioni\",\"minimi\",\"ministro\",\"mio\",\"modo\",\"molta\",\"molti\",\"moltissimo\",\"molto\",\"momento\",\"mondo\",\"mosto\",\"nazionale\",\"ne\",\"negl\",\"negli\",\"nei\",\"nel\",\"nell\",\"nella\",\"nelle\",\"nello\",\"nemmeno\",\"neppure\",\"nessun\",\"nessuna\",\"nessuno\",\"niente\",\"no\",\"noi\",\"nome\",\"non\",\"nondimeno\",\"nonostante\",\"nonsia\",\"nostra\",\"nostre\",\"nostri\",\"nostro\",\"novanta\",\"nove\",\"nulla\",\"nuovi\",\"nuovo\",\"o\",\"od\",\"oggi\",\"ogni\",\"ognuna\",\"ognuno\",\"oltre\",\"oppure\",\"ora\",\"ore\",\"osi\",\"ossia\",\"ottanta\",\"otto\",\"paese\",\"parecchi\",\"parecchie\",\"parecchio\",\"parte\",\"partendo\",\"peccato\",\"peggio\",\"per\",\"perche\",\"perchã¨\",\"perchè\",\"perché\",\"percio\",\"perciã²\",\"perciò\",\"perfino\",\"pero\",\"persino\",\"persone\",\"perã²\",\"però\",\"piedi\",\"pieno\",\"piglia\",\"piu\",\"piuttosto\",\"piã¹\",\"più\",\"po\",\"pochissimo\",\"poco\",\"poi\",\"poiche\",\"possa\",\"possedere\",\"posteriore\",\"posto\",\"potrebbe\",\"preferibilmente\",\"presa\",\"press\",\"prima\",\"primo\",\"principalmente\",\"probabilmente\",\"promesso\",\"proprio\",\"puo\",\"pure\",\"purtroppo\",\"puã²\",\"può\",\"qua\",\"qualche\",\"qualcosa\",\"qualcuna\",\"qualcuno\",\"quale\",\"quali\",\"qualunque\",\"quando\",\"quanta\",\"quante\",\"quanti\",\"quanto\",\"quantunque\",\"quarto\",\"quasi\",\"quattro\",\"quel\",\"quella\",\"quelle\",\"quelli\",\"quello\",\"quest\",\"questa\",\"queste\",\"questi\",\"questo\",\"qui\",\"quindi\",\"quinto\",\"realmente\",\"recente\",\"recentemente\",\"registrazione\",\"relativo\",\"riecco\",\"rispetto\",\"salvo\",\"sara\",\"sarai\",\"saranno\",\"sarebbe\",\"sarebbero\",\"sarei\",\"saremmo\",\"saremo\",\"sareste\",\"saresti\",\"sarete\",\"sarà\",\"sarã\",\"sarò\",\"scola\",\"scopo\",\"scorso\",\"se\",\"secondo\",\"seguente\",\"seguito\",\"sei\",\"sembra\",\"sembrare\",\"sembrato\",\"sembrava\",\"sembri\",\"sempre\",\"senza\",\"sette\",\"si\",\"sia\",\"siamo\",\"siano\",\"siate\",\"siete\",\"sig\",\"solito\",\"solo\",\"soltanto\",\"sono\",\"sopra\",\"soprattutto\",\"sotto\",\"spesso\",\"srl\",\"sta\",\"stai\",\"stando\",\"stanno\",\"starai\",\"staranno\",\"starebbe\",\"starebbero\",\"starei\",\"staremmo\",\"staremo\",\"stareste\",\"staresti\",\"starete\",\"starà\",\"starò\",\"stata\",\"state\",\"stati\",\"stato\",\"stava\",\"stavamo\",\"stavano\",\"stavate\",\"stavi\",\"stavo\",\"stemmo\",\"stessa\",\"stesse\",\"stessero\",\"stessi\",\"stessimo\",\"stesso\",\"steste\",\"stesti\",\"stette\",\"stettero\",\"stetti\",\"stia\",\"stiamo\",\"stiano\",\"stiate\",\"sto\",\"su\",\"sua\",\"subito\",\"successivamente\",\"successivo\",\"sue\",\"sugl\",\"sugli\",\"sui\",\"sul\",\"sull\",\"sulla\",\"sulle\",\"sullo\",\"suo\",\"suoi\",\"tale\",\"tali\",\"talvolta\",\"tanto\",\"te\",\"tempo\",\"terzo\",\"th\",\"ti\",\"titolo\",\"torino\",\"tra\",\"tranne\",\"tre\",\"trenta\",\"triplo\",\"troppo\",\"trovato\",\"tu\",\"tua\",\"tue\",\"tuo\",\"tuoi\",\"tutta\",\"tuttavia\",\"tutte\",\"tutti\",\"tutto\",\"uguali\",\"ulteriore\",\"ultimo\",\"un\",\"una\",\"uno\",\"uomo\",\"va\",\"vai\",\"vale\",\"vari\",\"varia\",\"varie\",\"vario\",\"verso\",\"vi\",\"via\",\"vicino\",\"visto\",\"vita\",\"voi\",\"volta\",\"volte\",\"vostra\",\"vostre\",\"vostri\",\"vostro\",\"ã¨\",\"è\"]\n",
    "\n",
    "words.append(\"hotel\")\n",
    "words.append(\"albergo\")\n",
    "words.append(\"stanza\")\n",
    "words.append(\"camera\")\n",
    "words.append(\"soggiorno\")\n",
    "words.append(\"notte\")\n",
    "words.append(\"giorno\")\n",
    "words.append(\"ristorante\")\n",
    "words.append(\"stelle\")\n",
    "\n",
    "words.extend((sw.words(\"italian\")))\n",
    "words = [ItalianStemmer.stem(w) for w in words]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "7aafkh-4buYW",
    "outputId": "fcc070d0-114d-49f4-cc4a-e1ae86b87743"
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "X_test = []\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "with open('development.csv', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "\n",
    "        X.append(row[\"text\"])\n",
    "        if row[\"class\"] == 'pos':\n",
    "            y.append(1)\n",
    "        if  row[\"class\"] == 'neg':\n",
    "            y.append(0)\n",
    "\n",
    "print(len(X))\n",
    "print(len(y))\n",
    "\n",
    "with open('evaluation.csv', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        X_test.append(row[\"text\"])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMj-3HQ9BzoH"
   },
   "outputs": [],
   "source": [
    "# X, X_va, y, y_va  = train_test_split(X, y, test_size=0.80, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "jPT4QMt5bxcN",
    "outputId": "62401025-8163-469f-8c6b-6ace8756a294"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "#we divide the dataset into training and val set\n",
    "\n",
    "\n",
    "lemmaTokenizer = LemmaTokenizer()\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmaTokenizer,stop_words=words, max_df=0.90 , min_df=25,ngram_range=(1,2),sublinear_tf = True)\n",
    "\n",
    "vectorizer.fit(X)\n",
    "X_train = vectorizer.transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "V-ggiQKnb0E2",
    "outputId": "4dcc2d2b-d930-4c8a-ca4e-726b8fca5c8c"
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "bM2iJH7QHmKH",
    "outputId": "4cb6fe05-f9b3-4a74-bfd4-d8afdb9fdb5f"
   },
   "outputs": [],
   "source": [
    "print(len(vectorizer.get_feature_names()))\n",
    "\n",
    "weights = np.asarray(X_train.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': vectorizer.get_feature_names(), 'weight': weights})\n",
    "weights_df.sort_values(by='weight', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(np.shape(X_train))\n",
    "print(len(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8OgwD1EA03t"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val  = train_test_split(X_train, y, test_size=0.2, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "020h66GyElQN"
   },
   "source": [
    "PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "mKc3ysWfb3cZ",
    "outputId": "d646bebc-42be-40a0-f5e9-203c4d7b95f7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=50,random_state=42)\n",
    "X_train_pca = svd.fit_transform(X_train)\n",
    "X_val_pca =svd.transform(X_val)\n",
    "# # X_test = svd.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(X_svd [:,0],X_svd[:,1], c=y_train)\n",
    "\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SVM_gridsearch(X_train_pca,X_val_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(X_train.shape[1]-1, random_state=42)\n",
    "X_svd = svd.fit_transform(X_train)\n",
    "cum_variance = np.cumsum(svd.explained_variance_ratio_)\n",
    "idx = np.argmax(cum_variance > .80)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svd = TruncatedSVD(n_components=idx,random_state=42)\n",
    "X_train_pca = svd.fit_transform(X_train)\n",
    "X_val_pca =svd.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C = 10, gamma = 1,  kernel='rbf')\n",
    "clf.fit(X_train_pca,y_train)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "y_true, y_pred = y_val, clf.predict(X_val_pca)\n",
    "score = f1_score(y_true, y_pred, average='weighted')\n",
    "print('validation score =', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "6fM4P79-gZla",
    "outputId": "2f1e1ec1-96ef-4762-aa51-c66e66541438"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train_tot = np.concatenate((X_train_pca, X_val_pca))\n",
    "y_train_tot = np.concatenate((y_train, y_val))\n",
    "\n",
    "clf = svm.SVC(C = 10, gamma = 1,  kernel='rbf')\n",
    "clf.fit(X_train_tot,y_train_tot)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j5nIC0IZIsy4"
   },
   "outputs": [],
   "source": [
    "X_test_tfid = vectorizer.transform(X_test)\n",
    "\n",
    "X_test_pca = svd.transform(X_test_tfid)\n",
    "\n",
    "\n",
    "Y_predicted = clf.predict(X_test_pca) # predict evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3nFY-OBL8OZ"
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "for y in Y_predicted:\n",
    "    if y == 1:\n",
    "        pred.append('pos')\n",
    "    else: \n",
    "        pred.append('neg')\n",
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mHHUvE0RLYxn"
   },
   "outputs": [],
   "source": [
    "# import numpy\n",
    "filenames =np.arange(len(Y_predicted))\n",
    "\n",
    "# a = numpy.asarray( [filenames, pred])\n",
    "# numpy.savetxt(\"svm_result.csv\", a.transpose(), delimiter=\",\")\n",
    "\n",
    "# import csv\n",
    "d = {\"Id\": filenames, \"Predicted\": pred}\n",
    "# with open(\"out.csv\", \"w\", newline=\"\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerows(pred)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(d)\n",
    "df.to_csv('result_c6.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
